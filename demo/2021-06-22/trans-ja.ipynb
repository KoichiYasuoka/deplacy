{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"世界のUniversal Dependenciesと係り受け解析ツール群" },
    "kernelspec":{ "name":"python3" },
    "accelerator": "GPU"
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [世界のUniversal Dependenciesと係り受け解析ツール群](http://kanji.zinbun.kyoto-u.ac.jp/~yasuoka/publications/2021-06-22.pdf)\n",
        "## [日本語UDを用いた係り受け解析器の自作](https://koichiyasuoka.github.io/deplacy/demo/2021-06-22/)\n",
        "### [Transformers](https://huggingface.co/transformers)と[bert-large-japanese-char-extended](https://huggingface.co/KoichiYasuoka/bert-large-japanese-char-extended)を用いる場合\n"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "必要なパッケージと訓練用train.conlluを準備"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!test -d UD_Japanese-GSD || git clone --depth=1 https://github.com/universaldependencies/UD_Japanese-GSD\n",
        "!test -f train.conllu || ln -s UD_Japanese-GSD/ja_gsd-ud-train.conllu train.conllu\n",
        "!pip install transformers datasets deplacy"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "train-c.conlluをtrain.conlluから作成"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "with open(\"train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "  r = f.read()\n",
        "with open(\"train-c.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "  for u in r.strip().split(\"\\n\\n\"):\n",
        "    w = [t for t in u.split(\"\\n\") if t.startswith(\"# text = \")][0]\n",
        "    s = [t.split(\"\\t\") for t in u.split(\"\\n\") if not t.startswith(\"#\")]\n",
        "    for i,v in enumerate([v for v in w[9:] if not v.isspace()]):\n",
        "      t = s[i]\n",
        "      x,t[0],t[1],t[2],t[8] = t[1],str(i+1),v,\"_\",\"_\"\n",
        "      t[9] = \"_\" if t[9].find(\"SpaceAfter=No\") < 0 else \"SpaceAfter=No\"\n",
        "      if v != x:\n",
        "        s.insert(i+1, [str(i+2),x[1:],\"_\",\"X\",\"_\",\"_\",t[6]\n",
        "           if t[7] == \"goeswith\" else str(i+1),\"goeswith\",\"_\",t[9]])\n",
        "        t[9] = \"SpaceAfter=No\"\n",
        "        for t in [t for t in s if int(t[6]) > i+1]:\n",
        "          t[6] = str(int(t[6])+1)\n",
        "    print(w, \"\\n\".join(\"\\t\".join(t) for t in s), \"\", sep=\"\\n\", file=f)"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "my.transを作成 (GPUで1時間程度)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "from transformers import (AutoTokenizer, AutoConfig,\n",
        "  AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
        "  TrainingArguments, Trainer)\n",
        "from datasets.arrow_dataset import Dataset\n",
        "brt = \"KoichiYasuoka/bert-large-japanese-char-extended\"\n",
        "with open(\"train-c.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "  tok,tag = [],[]\n",
        "  for s in f.read().strip().split(\"\\n\\n\"):\n",
        "    v = [t.split(\"\\t\") for t in s.split(\"\\n\") if not t.startswith(\"#\")]\n",
        "    tok.append([t[1] for t in v])\n",
        "    tag.append([\"\\t\".join(t[3:6]+[\"{:+}\".format(int(t[6])-int(t[0]))\n",
        "      if int(t[6]) else \"0\", t[7]]) for t in v])\n",
        "lid = {l:i for i,l in enumerate(set(sum(tag, [])))}\n",
        "tkz = AutoTokenizer.from_pretrained(brt)\n",
        "dts = Dataset.from_dict({\"tokens\": tok, \"tags\": tag,\n",
        "  \"input_ids\": [tkz.convert_tokens_to_ids(s) for s in tok],\n",
        "  \"labels\": [[lid[t] for t in s] for s in tag]})\n",
        "cfg = AutoConfig.from_pretrained(brt, num_labels=len(lid), label2id=lid,\n",
        "  id2label={i:l for l,i in lid.items()})\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(brt, config=cfg)\n",
        "dcl = DataCollatorForTokenClassification(tokenizer=tkz)\n",
        "arg = TrainingArguments(output_dir=\"/tmp\", overwrite_output_dir=True,\n",
        "  per_device_train_batch_size=4, save_total_limit=2)\n",
        "trn = Trainer(model=mdl, args=arg, data_collator=dcl, train_dataset=dts)\n",
        "trn.train()\n",
        "trn.save_model(\"my.trans\")\n",
        "tkz.save_pretrained(\"my.trans\")"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "my.transで係り受け解析"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tkz = AutoTokenizer.from_pretrained(\"my.trans\")\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(\"my.trans\")\n",
        "def nlp(sentence):\n",
        "  s = [t for t in sentence if not t.isspace()]\n",
        "  t = [i for i,t in enumerate(sentence) if t.isspace()]\n",
        "  m = [i-j-1 for j,i in enumerate(t)]\n",
        "  e = tkz.encode(s, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  for i,q in enumerate(torch.argmax(mdl(e)[0], dim=2)[0].tolist()):\n",
        "    t = mdl.config.id2label[q].split(\"\\t\")\n",
        "    t[3] = str(int(t[3])+i+1) if int(t[3]) else \"0\"\n",
        "    s[i] = [s[i],\"_\"]+t+[\"_\",\"_\" if i in m else \"SpaceAfter=No\"]\n",
        "  return \"\\n\".join(\"\\t\".join([str(i+1)]+t) for i,t in enumerate(s))+\"\\n\\n\"\n",
        "doc=nlp(\"虎穴に入らざれば虎子を得ず。\")\n",
        "print(doc)\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "goeswithを削り取る"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tkz = AutoTokenizer.from_pretrained(\"my.trans\")\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(\"my.trans\")\n",
        "def nlp(sentence):\n",
        "  s = [t for t in sentence if not t.isspace()]\n",
        "  t = [i for i,t in enumerate(sentence) if t.isspace()]\n",
        "  m = [i-j-1 for j,i in enumerate(t)]\n",
        "  e = tkz.encode(s, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  for i,q in enumerate(torch.argmax(mdl(e)[0], dim=2)[0].tolist()):\n",
        "    t = mdl.config.id2label[q].split(\"\\t\")\n",
        "    t[3] = str(int(t[3])+i+1) if int(t[3]) else \"0\"\n",
        "    s[i] = [s[i],\"_\"]+t+[\"_\",\"_\" if i in m else \"SpaceAfter=No\"]\n",
        "  for i in [i for i in range(len(s)-1, 0, -1) if s[i][6] == \"goeswith\"]:\n",
        "    t = s.pop(i)\n",
        "    s[i-1][0],s[i-1][8] = s[i-1][0]+t[0],t[8]\n",
        "    for t in [t for t in s if int(t[5]) > i]:\n",
        "      t[5] = str(int(t[5])-1)\n",
        "  return \"\\n\".join(\"\\t\".join([str(i+1)]+t) for i,t in enumerate(s))+\"\\n\\n\"\n",
        "doc=nlp(\"虎穴に入らざれば虎子を得ず。\")\n",
        "print(doc)\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    }
  ]
}
