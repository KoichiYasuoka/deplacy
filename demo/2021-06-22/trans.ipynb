{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"世界のUniversal Dependenciesと係り受けツール群" },
    "kernelspec":{ "name":"python3" },
    "accelerator": "GPU"
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [世界のUniversal Dependenciesと係り受けツール群](http://kanji.zinbun.kyoto-u.ac.jp/~yasuoka/publications/2021-06-22.pdf)\n",
        "## 日本語UDを用いた係り受け解析器の自作\n",
        "### Transformersとbert-large-japanese-char-extendedを用いる場合\n"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "必要なパッケージと訓練用train.conlluを準備"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!test -d UD_Japanese-GSD || git clone --depth=1 https://github.com/universaldependencies/UD_Japanese-GSD\n",
        "!test -f train.conllu || ln -s UD_Japanese-GSD/ja_gsd-ud-train.conllu train.conllu\n",
        "!pip install transformers datasets deplacy"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "train-c.conlluをtrain.conlluから作成"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "from transformers import AutoTokenizer as AT\n",
        "tkz = AT.from_pretrained(\"KoichiYasuoka/bert-large-japanese-char-extended\")\n",
        "with open(\"train.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "  r = f.read()\n",
        "with open(\"train-c.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "  u,h = [],[0]\n",
        "  for s in r.split(\"\\n\"):\n",
        "    if s.startswith(\"# text = \"):\n",
        "      v,w = tkz.tokenize(s[9:]),s\n",
        "    elif s > \"\" and not s.startswith(\"#\"):\n",
        "      t = s.split(\"\\t\")\n",
        "      m = \"_\" if t[9].find(\"SpaceAfter=No\") < 0 else \"SpaceAfter=No\"\n",
        "      x,t[2],t[6],t[8],t[9] = t[1],\"_\",int(t[6]),\"_\",\"SpaceAfter=No\"\n",
        "      h.append(len(u)+1)\n",
        "      while x > \"\":\n",
        "        t[1],x = v[len(u)],x[len(v[len(u)].replace(\"##\", \"\")):]\n",
        "        u.append(list(t))\n",
        "        t[3:8] = \"X\",\"_\",\"_\",int(t[0]),\"goeswith\"\n",
        "      u[-1][9] = m\n",
        "    elif s == \"\" and len(u) > 0:\n",
        "      print(w, \"\\n\".join(\"\\t\".join([str(i+1)]+t[1:6]+[str(h[t[6]])]+t[7:])\n",
        "        for i,t in enumerate(u)), \"\", sep=\"\\n\", file=f)\n",
        "      u,h = [],[0]"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "my.transを作成"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "from transformers import (AutoTokenizer, AutoConfig,\n",
        "  AutoModelForTokenClassification, DataCollatorForTokenClassification,\n",
        "  TrainingArguments, Trainer)\n",
        "from datasets.arrow_dataset import Dataset\n",
        "brt = \"KoichiYasuoka/bert-large-japanese-char-extended\"\n",
        "with open(\"train-c.conllu\", \"r\", encoding=\"utf-8\") as f:\n",
        "  tok,tag = [],[]\n",
        "  for s in f.read().strip().split(\"\\n\\n\"):\n",
        "    v = [t.split(\"\\t\") for t in s.split(\"\\n\") if not t.startswith(\"#\")]\n",
        "    tok.append([t[1] for t in v])\n",
        "    tag.append([\"\\t\".join([t[3], t[4], t[5], (\"{:+}\" if int(t[6]) else \"0\")\n",
        "      .format(int(t[6])-int(t[0])), t[7]]) for t in v])\n",
        "lid = {l:i for i,l in enumerate(set(sum(tag, [])))}\n",
        "tkz = AutoTokenizer.from_pretrained(brt)\n",
        "dts = Dataset.from_dict({\"tokens\": tok, \"tags\": tag,\n",
        "  \"input_ids\": [tkz.convert_tokens_to_ids(s) for s in tok],\n",
        "  \"labels\": [[lid[t] for t in s] for s in tag]})\n",
        "cfg = AutoConfig.from_pretrained(brt, num_labels=len(lid), label2id=lid,\n",
        "  id2label={i:l for l,i in lid.items()})\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(brt, config=cfg)\n",
        "dcl = DataCollatorForTokenClassification(tokenizer=tkz)\n",
        "arg = TrainingArguments(output_dir=\"/tmp\", overwrite_output_dir=True,\n",
        "  per_device_train_batch_size=4)\n",
        "trn = Trainer(model=mdl, args=arg, data_collator=dcl, train_dataset=dts)\n",
        "trn.train()\n",
        "trn.save_model(\"my.trans\")\n",
        "tkz.save_pretrained(\"my.trans\")"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "my.transで係り受け解析"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tkz = AutoTokenizer.from_pretrained(\"my.trans\")\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(\"my.trans\")\n",
        "def nlp(sentence):\n",
        "  s = tkz.tokenize(sentence)\n",
        "  e = tkz.encode(s, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  for i,q in enumerate(torch.argmax(mdl(e)[0], dim=2)[0].tolist()):\n",
        "    t = [s[i],\"_\"]+mdl.config.id2label[q].split(\"\\t\")+[\"_\",\"SpaceAfter=No\"]\n",
        "    s[i] = t[0:5]+[str(int(t[5])+i+1) if int(t[5]) else \"0\"]+t[6:]\n",
        "  return \"\\n\".join(\"\\t\".join([str(i+1)]+t) for i,t in enumerate(s))+\"\\n\\n\"\n",
        "doc=nlp(\"虎穴に入らざれば虎子を得ず。\")\n",
        "print(doc)\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "goeswithを削り取る"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "tkz = AutoTokenizer.from_pretrained(\"my.trans\")\n",
        "mdl = AutoModelForTokenClassification.from_pretrained(\"my.trans\")\n",
        "def nlp(sentence):\n",
        "  s = tkz.tokenize(sentence)\n",
        "  e = tkz.encode(s, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  for i,q in enumerate(torch.argmax(mdl(e)[0], dim=2)[0].tolist()):\n",
        "    t = [s[i],\"_\"]+mdl.config.id2label[q].split(\"\\t\")+[\"_\",\"SpaceAfter=No\"]\n",
        "    s[i] = t[0:5]+[str(int(t[5])+i+1) if int(t[5]) else \"0\"]+t[6:]\n",
        "  for i in [i for i in range(len(s)-1, 0, -1) if s[i][6] == \"goeswith\"]:\n",
        "    t = s.pop(i)\n",
        "    s[i-1][0] += t[0][2:] if t[0].startswith(\"##\") else t[0]\n",
        "    for t in [t for t in s if int(t[5]) > i]:\n",
        "      t[5] = str(int(t[5])-1)\n",
        "  return \"\\n\".join(\"\\t\".join([str(i+1)]+t) for i,t in enumerate(s))+\"\\n\\n\"\n",
        "doc=nlp(\"虎穴に入らざれば虎子を得ず。\")\n",
        "print(doc)\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    }
  ]
}
