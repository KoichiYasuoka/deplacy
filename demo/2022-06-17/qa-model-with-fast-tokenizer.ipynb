{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"TransformersのQuestion Answeringを用いた係り受け解析器" },
    "kernelspec":{ "name":"python3" },
    "accelerator": "GPU"
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# TransformersのQuestion Answeringを用いた係り受け解析器\n",
        "## モデル簡易作成ツール(fastトークナイザ向け)\n"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "必要なパッケージと各conlluを準備"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import os\n",
        "url=\"https://github.com/UniversalDependencies/UD_Japanese-GSDLUW\"\n",
        "d=os.path.basename(url)\n",
        "!test -d $d || git clone --depth=1 $url\n",
        "!for F in train dev test ; do cat $d/*-$$F.conllu > $$F.conllu ; done\n",
        "url=\"https://github.com/huggingface/transformers\"\n",
        "d=os.path.basename(url)\n",
        "!pip install $d pytokenizations ufal.chu-liu-edmonds datasets seqeval\n",
        "!test -d $d || git clone --depth=1 -b `pip list | sed -n \"s/^$d */v/p\"` $url\n",
        "url=\"https://universaldependencies.org/conll18/conll18_ud_eval.py\"\n",
        "f=os.path.basename(url)\n",
        "!test -f $f || curl -LO $url"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "qa-model作成(HEAD返答モデル)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import tokenizations,json\n",
        "from transformers import AutoTokenizer\n",
        "src=\"KoichiYasuoka/deberta-base-japanese-aozora\"\n",
        "m,n=AutoTokenizer.from_pretrained(src).mask_token,0\n",
        "for f in [\"train\",\"dev\",\"test\"]:\n",
        "  u,v,w=[],None,[]\n",
        "  with open(f+\".conllu\",\"r\",encoding=\"utf-8\") as r:\n",
        "    for s in r.read().split(\"\\n\"):\n",
        "      if s.startswith(\"# text = \"):\n",
        "        v=s[9:]\n",
        "      elif s==\"\":\n",
        "        if v and w!=[]:\n",
        "          v2w,w2v=tokenizations.get_alignments(v,[x[1] for x in w])\n",
        "          for i,t in enumerate([] if (w2v+[[]]).index([])<len(w2v) else w):\n",
        "            n,c,h=n+1,v[0:w2v[i][0]]+m+v[w2v[i][-1]+1:],int(t[6])-1\n",
        "            s=w2v[i if h<0 else h][0]+(len(m)-len(t[1]) if i<h else 0)\n",
        "            a={\"text\":[m if h<0 else w[h][1]],\"answer_start\":[s]}\n",
        "            u.append({\"id\":n,\"context\":c,\"question\":t[1],\"answers\":a})\n",
        "            if f==\"train\":\n",
        "              if [1 for x in w if x[1]==t[1]]==[1]:\n",
        "                n,j=n+1,i if h<0 else h\n",
        "                a={\"text\":[w[j][1]],\"answer_start\":[w2v[j][0]]}\n",
        "                u.append({\"id\":n,\"context\":v,\"question\":t[1],\"answers\":a})\n",
        "        v,w=None,[]\n",
        "      else:\n",
        "        t=s.split(\"\\t\")\n",
        "        if len(t)==10 and t[0].isdecimal():\n",
        "          w.append(t)\n",
        "  with open(f+\".json\",\"w\",encoding=\"utf-8\") as w:\n",
        "    json.dump({\"data\":u},w,ensure_ascii=False,indent=2)\n",
        "s=\" \".join([\"transformers/examples/pytorch/question-answering/run_qa.py\",\n",
        "  \"--model_name_or_path=\"+src,\n",
        "  \"--output_dir=qa-model --overwrite_output_dir --save_total_limit=2\",\n",
        "  \"--per_device_train_batch_size=16 --per_device_eval_batch_size=16\",\n",
        "  \"--train_file=train.json --validation_file=dev.json --test_file=test.json\",\n",
        "  \"--do_train --do_eval --do_predict --evaluation_strategy=epoch\",\n",
        "  \"--learning_rate=5e-05 --num_train_epochs=3 --warmup_ratio=0.1\"])\n",
        "!python $s"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "qa-model/deprel作成(DEPREL付与モデル)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import json\n",
        "v,y=set(),set()\n",
        "for f in [\"test\",\"dev\",\"train\"]:\n",
        "  u=[]\n",
        "  with open(f+\".conllu\",\"r\",encoding=\"utf-8\") as r:\n",
        "    for s in r.read().split(\"\\n\\n\"):\n",
        "      a=[x.split(\"\\t\") for x in s.split(\"\\n\")]\n",
        "      b=[x for x in a if len(x)==10 and x[7]!=\"_\"]\n",
        "      if b!=[]:\n",
        "        t,p=[x[1] for x in b],[\"B-\"+x[7] for x in b]\n",
        "        u.append({\"tokens\":t,\"pos\":p})\n",
        "        if f==\"train\":\n",
        "          v-=set(p)\n",
        "        else:\n",
        "          v|=set(p)\n",
        "        y|=set([j for i,j in zip(t,p) if len(i)>1])\n",
        "  if f==\"train\":\n",
        "    u.append({\"tokens\":[\"〓\"]*(len(v|y)),\"pos\":list(v)+[\"I-\"+x[2:] for x in y]})\n",
        "  with open(f+\".json\",\"w\",encoding=\"utf-8\") as w:\n",
        "    for t in u:\n",
        "      print(json.dumps(t,ensure_ascii=False),file=w)\n",
        "s=\" \".join([\"transformers/examples/pytorch/token-classification/run_ner.py\",\n",
        "  \"--model_name_or_path=qa-model --task_name=pos --label_all_tokens\",\n",
        "  \"--output_dir=qa-model/deprel --overwrite_output_dir --save_total_limit=2\",\n",
        "  \"--train_file=train.json --validation_file=dev.json --test_file=test.json\",\n",
        "  \"--do_train --do_eval --do_predict --evaluation_strategy=epoch\",\n",
        "  \"--learning_rate=5e-05 --num_train_epochs=3 --warmup_ratio=0.1\"])\n",
        "!python $s"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "qa-model/tagger作成(UPOS・FEATS付与モデル)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[

        "import json\n",
        "v,y=set(),set()\n",
        "for f in [\"test\",\"dev\",\"train\"]:\n",
        "  u=[]\n",
        "  with open(f+\".conllu\",\"r\",encoding=\"utf-8\") as r:\n",
        "    for s in r.read().split(\"\\n\\n\"):\n",
        "      a=[x.split(\"\\t\") for x in s.split(\"\\n\")]\n",
        "      b=[x for x in a if len(x)==10 and x[3]!=\"_\"]\n",
        "      if b!=[]:\n",
        "        t,p=[x[1] for x in b],[\"B-\"+x[3]+\"|\"+x[5] for x in b]\n",
        "        u.append({\"tokens\":t,\"pos\":p})\n",
        "        if f==\"train\":\n",
        "          v-=set(p)\n",
        "        else:\n",
        "          v|=set(p)\n",
        "        y|=set([j for i,j in zip(t,p) if len(i)>1])\n",
        "  if f==\"train\":\n",
        "    u.append({\"tokens\":[\"〓\"]*(len(v|y)),\"pos\":list(v)+[\"I-\"+x[2:] for x in y]})\n",
        "  with open(f+\".json\",\"w\",encoding=\"utf-8\") as w:\n",
        "    for t in u:\n",
        "      print(json.dumps(t,ensure_ascii=False),file=w)\n",
        "s=\" \".join([\"transformers/examples/pytorch/token-classification/run_ner.py\",\n",
        "  \"--model_name_or_path=qa-model --task_name=pos --label_all_tokens\",\n",
        "  \"--output_dir=qa-model/tagger --overwrite_output_dir --save_total_limit=2\",\n",
        "  \"--train_file=train.json --validation_file=dev.json --test_file=test.json\",\n",
        "  \"--do_train --do_eval --do_predict --evaluation_strategy=epoch\",\n",
        "  \"--learning_rate=5e-05 --num_train_epochs=3 --warmup_ratio=0.1\"])\n",
        "!python $s"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "評価とテスト"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "class TransformersUD(object):\n",
        "  def __init__(self,bert):\n",
        "    import os\n",
        "    from transformers import (AutoTokenizer,AutoModelForQuestionAnswering,\n",
        "      AutoModelForTokenClassification,AutoConfig,TokenClassificationPipeline)\n",
        "    self.tokenizer=AutoTokenizer.from_pretrained(bert)\n",
        "    self.model=AutoModelForQuestionAnswering.from_pretrained(bert)\n",
        "    x=AutoModelForTokenClassification.from_pretrained\n",
        "    if os.path.isdir(bert):\n",
        "      d,t=x(os.path.join(bert,\"deprel\")),x(os.path.join(bert,\"tagger\"))\n",
        "    else:\n",
        "      from transformers.file_utils import hf_bucket_url\n",
        "      c=AutoConfig.from_pretrained(hf_bucket_url(bert,\"deprel/config.json\"))\n",
        "      d=x(hf_bucket_url(bert,\"deprel/pytorch_model.bin\"),config=c)\n",
        "      s=AutoConfig.from_pretrained(hf_bucket_url(bert,\"tagger/config.json\"))\n",
        "      t=x(hf_bucket_url(bert,\"tagger/pytorch_model.bin\"),config=s)\n",
        "    self.deprel=TokenClassificationPipeline(model=d,tokenizer=self.tokenizer,\n",
        "      aggregation_strategy=\"simple\")\n",
        "    self.tagger=TokenClassificationPipeline(model=t,tokenizer=self.tokenizer)\n",
        "  def __call__(self,text):\n",
        "    import numpy,torch,ufal.chu_liu_edmonds\n",
        "    w=[(t[\"start\"],t[\"end\"],t[\"entity_group\"]) for t in self.deprel(text)]\n",
        "    z,n={t[\"start\"]:t[\"entity\"].split(\"|\") for t in self.tagger(text)},len(w)\n",
        "    r,m=[text[s:e] for s,e,p in w],numpy.full((n+1,n+1),numpy.nan)\n",
        "    v=self.tokenizer(r,add_special_tokens=False)[\"input_ids\"]\n",
        "    for i,t in enumerate(v):\n",
        "      q=[self.tokenizer.cls_token_id]+t+[self.tokenizer.sep_token_id]\n",
        "      c=[q]+v[0:i]+[[self.tokenizer.mask_token_id]]+v[i+1:]+[[q[-1]]]\n",
        "      b=[len(sum(c[0:j+1],[])) for j in range(len(c))]\n",
        "      d=self.model(input_ids=torch.tensor([sum(c,[])]),\n",
        "        token_type_ids=torch.tensor([[0]*b[0]+[1]*(b[-1]-b[0])]))\n",
        "      s,e=d.start_logits.tolist()[0],d.end_logits.tolist()[0]\n",
        "      for j in range(n):\n",
        "        m[i+1,0 if i==j else j+1]=s[b[j]]+e[b[j+1]-1]\n",
        "    h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "    u=\"# text = \"+text.replace(\"\\n\",\" \")+\"\\n\"\n",
        "    for i,(s,e,p) in enumerate(w,1):\n",
        "      u+=\"\\t\".join([str(i),r[i-1],\"_\",z[s][0][2:],\"_\",\"|\".join(z[s][1:]),\n",
        "        str(h[i]),p,\"_\",\"_\" if i<n and w[i][0]<e else \"SpaceAfter=No\"])+\"\\n\"\n",
        "    return u+\"\\n\"\n",
        "\n",
        "nlp=TransformersUD(\"qa-model\")\n",
        "for f in [\"dev.conllu\",\"test.conllu\"]:\n",
        "  with open(f,\"r\",encoding=\"utf-8\") as r:\n",
        "    with open(\"result-\"+f,\"w\",encoding=\"utf-8\") as w:\n",
        "      for s in r.read().split(\"\\n\"):\n",
        "        if s.startswith(\"# text = \"):\n",
        "          w.write(nlp(s[9:]))\n",
        "print(\"*** dev.conllu\")\n",
        "!python conll18_ud_eval.py -v dev.conllu result-dev.conllu\n",
        "print(\"*** test.conllu\")\n",
        "!python conll18_ud_eval.py -v test.conllu result-test.conllu"
      ]
    }
  ]
}
