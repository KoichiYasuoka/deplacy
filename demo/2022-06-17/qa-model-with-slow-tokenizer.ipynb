{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"TransformersのQuestion Answeringを用いた係り受け解析器" },
    "kernelspec":{ "name":"python3" },
    "accelerator": "GPU"
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [TransformersのQuestion Answeringを用いた係り受け解析器](https://koichiyasuoka.github.io/deplacy/demo/2022-06-17/)\n",
        "## モデル簡易作成ツール(slowトークナイザ向け)\n"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "必要なパッケージと各conlluを準備"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import os\n",
        "url=\"https://github.com/UniversalDependencies/UD_Japanese-GSDLUW\"\n",
        "d=os.path.basename(url)\n",
        "!test -d $d || git clone --depth=1 $url\n",
        "!for F in train dev test ; do cat $d/*-$$F.conllu > $$F.conllu ; done\n",
        "url=\"https://github.com/huggingface/transformers\"\n",
        "d=os.path.basename(url)\n",
        "!pip install $d pytokenizations ufal.chu-liu-edmonds datasets seqeval\n",
        "!pip install fugashi unidic-lite\n",
        "!test -d $d || git clone --depth=1 -b `pip list | sed -n \"s/^$d */v/p\"` $url\n",
        "url=\"https://universaldependencies.org/conll18/conll18_ud_eval.py\"\n",
        "f=os.path.basename(url)\n",
        "!test -f $f || curl -LO $url\n",
        "!test -f $f || curl -LO $url\n",
        "url=\"https://raw.githubusercontent.com/KoichiYasuoka/deplacy\"\n",
        "url+=\"/master/demo/2022-06-17/UD-KyotsuTest2022Kokugo\"\n",
        "!test -f question1-1.conllu || curl -LO $url/question1-1.conllu\n",
        "!test -f question1-2.conllu || curl -LO $url/question1-2.conllu"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "qa-model作成(HEAD返答モデル)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "import tokenizations,json\n",
        "from transformers import AutoTokenizer,AutoConfig\n",
        "src=\"KoichiYasuoka/deberta-base-japanese-unidic\"\n",
        "m,n=AutoTokenizer.from_pretrained(src).mask_token,0\n",
        "for f in [\"train\",\"dev\",\"test\"]:\n",
        "  u,v,w=[],None,[]\n",
        "  with open(f+\".conllu\",\"r\",encoding=\"utf-8\") as r:\n",
        "    for s in r.read().split(\"\\n\"):\n",
        "      if s.startswith(\"# text = \"):\n",
        "        v=s[9:]\n",
        "      elif s==\"\":\n",
        "        if v and w!=[]:\n",
        "          v2w,w2v=tokenizations.get_alignments(v,[x[1] for x in w])\n",
        "          for i,t in enumerate([] if (w2v+[[]]).index([])<len(w2v) else w):\n",
        "            n,c,h=n+1,v[0:w2v[i][0]]+m+v[w2v[i][-1]+1:],int(t[6])-1\n",
        "            s=w2v[i if h<0 else h][0]+(len(m)-len(t[1]) if i<h else 0)\n",
        "            u.append({\"context\":c,\"qas\":[{\"id\":n,\"question\":t[1],\n",
        "              \"answers\":[{\"text\":m if h<0 else w[h][1],\"answer_start\":s}]}]})\n",
        "            if f==\"train\":\n",
        "              if [1 for x in w if x[1]==t[1]]==[1]:\n",
        "                n,j=n+1,i if h<0 else h\n",
        "                u.append({\"context\":v,\"qas\":[{\"id\":n,\"question\":t[1],\n",
        "                  \"answers\":[{\"text\":w[j][1],\"answer_start\":w2v[j][0]}]}]})\n",
        "        v,w=None,[]\n",
        "      else:\n",
        "        t=s.split(\"\\t\")\n",
        "        if len(t)==10 and t[0].isdecimal():\n",
        "          w.append(t)\n",
        "  with open(f+\".json\",\"w\",encoding=\"utf-8\") as w:\n",
        "    json.dump({\"data\":[{\"title\":\"UD\",\"paragraphs\":u}]},w,ensure_ascii=False)\n",
        "t=AutoConfig.from_pretrained(src).model_type\n",
        "s=\" \".join([\"transformers/examples/legacy/question-answering/run_squad.py\",\n",
        "  \"--model_name_or_path=\"+src,\"--model_type=\"+t,\n",
        "  \"--output_dir=qa-model --overwrite_output_dir --save_steps=5000\",\n",
        "  \"--per_gpu_train_batch_size=16 --per_gpu_eval_batch_size=16\",\n",
        "  \"--train_file=train.json --predict_file=dev.json\",\n",
        "  \"--do_train --do_eval --learning_rate=5e-05 --num_train_epochs=3\"])\n",
        "!python $s"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "qa-model/deprel作成(DEPREL付与モデル)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "src=\"KoichiYasuoka/deberta-base-japanese-unidic\"\n",
        "class UPOSDataset(object):\n",
        "  def __init__(self,conllu,tokenizer,fields=[3]):\n",
        "    self.ids,self.upos=[],[]\n",
        "    label,cls,sep=set(),tokenizer.cls_token_id,tokenizer.sep_token_id\n",
        "    with open(conllu,\"r\",encoding=\"utf-8\") as r:\n",
        "      form,upos=[],[]\n",
        "      for t in r:\n",
        "        w=t.split(\"\\t\")\n",
        "        if len(w)==10 and w[0].isdecimal():\n",
        "          form.append(w[1])\n",
        "          upos.append(\"|\".join(w[i] for i in fields))\n",
        "        elif t.strip()==\"\" and form!=[]:\n",
        "          v,u=tokenizer(form,add_special_tokens=False)[\"input_ids\"],[]\n",
        "          for x,y in zip(v,upos):\n",
        "            u.extend([\"B-\"+y]*min(len(x),1)+[\"I-\"+y]*(len(x)-1))\n",
        "          if len(u)>tokenizer.model_max_length-4:\n",
        "            self.ids.append(sum(v,[])[0:tokenizer.model_max_length-2])\n",
        "            self.upos.append(u[0:tokenizer.model_max_length-2])\n",
        "          elif len(u)>0:\n",
        "            self.ids.append([cls]+sum(v,[])+[sep])\n",
        "            self.upos.append([u[0]]+u+[u[0]])\n",
        "          label=set(sum([self.upos[-1],list(label)],[]))\n",
        "          form,upos=[],[]\n",
        "    self.label2id={l:i for i,l in enumerate(sorted(label))}\n",
        "  def __call__(*args):\n",
        "    label=set(sum([list(t.label2id) for t in args],[]))\n",
        "    lid={l:i for i,l in enumerate(sorted(label))}\n",
        "    for t in args:\n",
        "      t.label2id=lid\n",
        "    return lid\n",
        "  __len__=lambda self:len(self.ids)\n",
        "  __getitem__=lambda self,i:{\"input_ids\":self.ids[i],\n",
        "    \"labels\":[self.label2id[t] for t in self.upos[i]]}\n",
        "from transformers import (AutoTokenizer,AutoModelForTokenClassification,\n",
        "  AutoConfig,DataCollatorForTokenClassification,TrainingArguments,Trainer)\n",
        "tkz=AutoTokenizer.from_pretrained(src)\n",
        "trainDS=UPOSDataset(\"train.conllu\",tkz,[7])\n",
        "devDS=UPOSDataset(\"dev.conllu\",tkz,[7])\n",
        "testDS=UPOSDataset(\"test.conllu\",tkz,[7])\n",
        "lid=trainDS(devDS,testDS)\n",
        "cfg=AutoConfig.from_pretrained(src,num_labels=len(lid),label2id=lid,\n",
        "  id2label={i:l for l,i in lid.items()})\n",
        "arg=TrainingArguments(num_train_epochs=3,per_device_train_batch_size=16,\n",
        "  output_dir=\"qa-model\",overwrite_output_dir=True,save_total_limit=2)\n",
        "trn=Trainer(args=arg,data_collator=DataCollatorForTokenClassification(tkz),\n",
        "  model=AutoModelForTokenClassification.from_pretrained(src,config=cfg),\n",
        "  train_dataset=trainDS,eval_dataset=devDS)\n",
        "trn.train()\n",
        "trn.save_model(\"qa-model/deprel\")\n",
        "tkz.save_pretrained(\"qa-model/deprel\")"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "qa-model/tagger作成(UPOS・FEATS付与モデル)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "src=\"KoichiYasuoka/deberta-base-japanese-unidic\"\n",
        "class UPOSDataset(object):\n",
        "  def __init__(self,conllu,tokenizer,fields=[3]):\n",
        "    self.ids,self.upos=[],[]\n",
        "    label,cls,sep=set(),tokenizer.cls_token_id,tokenizer.sep_token_id\n",
        "    with open(conllu,\"r\",encoding=\"utf-8\") as r:\n",
        "      form,upos=[],[]\n",
        "      for t in r:\n",
        "        w=t.split(\"\\t\")\n",
        "        if len(w)==10 and w[0].isdecimal():\n",
        "          form.append(w[1])\n",
        "          upos.append(\"|\".join(w[i] for i in fields))\n",
        "        elif t.strip()==\"\" and form!=[]:\n",
        "          v,u=tokenizer(form,add_special_tokens=False)[\"input_ids\"],[]\n",
        "          for x,y in zip(v,upos):\n",
        "            u.extend([\"B-\"+y]*min(len(x),1)+[\"I-\"+y]*(len(x)-1))\n",
        "          if len(u)>tokenizer.model_max_length-4:\n",
        "            self.ids.append(sum(v,[])[0:tokenizer.model_max_length-2])\n",
        "            self.upos.append(u[0:tokenizer.model_max_length-2])\n",
        "          elif len(u)>0:\n",
        "            self.ids.append([cls]+sum(v,[])+[sep])\n",
        "            self.upos.append([u[0]]+u+[u[0]])\n",
        "          label=set(sum([self.upos[-1],list(label)],[]))\n",
        "          form,upos=[],[]\n",
        "    self.label2id={l:i for i,l in enumerate(sorted(label))}\n",
        "  def __call__(*args):\n",
        "    label=set(sum([list(t.label2id) for t in args],[]))\n",
        "    lid={l:i for i,l in enumerate(sorted(label))}\n",
        "    for t in args:\n",
        "      t.label2id=lid\n",
        "    return lid\n",
        "  __len__=lambda self:len(self.ids)\n",
        "  __getitem__=lambda self,i:{\"input_ids\":self.ids[i],\n",
        "    \"labels\":[self.label2id[t] for t in self.upos[i]]}\n",
        "from transformers import (AutoTokenizer,AutoModelForTokenClassification,\n",
        "  AutoConfig,DataCollatorForTokenClassification,TrainingArguments,Trainer)\n",
        "tkz=AutoTokenizer.from_pretrained(src)\n",
        "trainDS=UPOSDataset(\"train.conllu\",tkz,[3,5])\n",
        "devDS=UPOSDataset(\"dev.conllu\",tkz,[3,5])\n",
        "testDS=UPOSDataset(\"test.conllu\",tkz,[3,5])\n",
        "lid=trainDS(devDS,testDS)\n",
        "cfg=AutoConfig.from_pretrained(src,num_labels=len(lid),label2id=lid,\n",
        "  id2label={i:l for l,i in lid.items()})\n",
        "arg=TrainingArguments(num_train_epochs=3,per_device_train_batch_size=64,\n",
        "  output_dir=\"qa-model\",overwrite_output_dir=True,save_total_limit=2)\n",
        "trn=Trainer(args=arg,data_collator=DataCollatorForTokenClassification(tkz),\n",
        "  model=AutoModelForTokenClassification.from_pretrained(src,config=cfg),\n",
        "  train_dataset=trainDS,eval_dataset=devDS)\n",
        "trn.train()\n",
        "trn.save_model(\"qa-model/tagger\")\n",
        "tkz.save_pretrained(\"qa-model/tagger\")"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "評価とテスト"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "from transformers import (AutoTokenizer,AutoModelForQuestionAnswering,\n",
        "  AutoModelForTokenClassification,AutoConfig,TokenClassificationPipeline)\n",
        "class TaggerPipeline(TokenClassificationPipeline):\n",
        "  def __call__(self,text):\n",
        "    d=super().__call__(text)\n",
        "    if len(d)>0 and (\"start\" not in d[0] or d[0][\"start\"]==None):\n",
        "      import tokenizations\n",
        "      v=[x[\"word\"].replace(\" \",\"\") for x in d]\n",
        "      a2b,b2a=tokenizations.get_alignments(v,text)\n",
        "      for i,t in enumerate(a2b):\n",
        "        s,e=(0,0) if t==[] else (t[0],t[-1]+1)\n",
        "        if v[i].startswith(self.tokenizer.unk_token):\n",
        "          s=([[-1]]+[x for x in a2b[0:i] if x>[]])[-1][-1]+1\n",
        "        if v[i].endswith(self.tokenizer.unk_token):\n",
        "          e=([x for x in a2b[i+1:] if x>[]]+[[len(text)]])[0][0]\n",
        "        d[i][\"start\"],d[i][\"end\"]=s,e\n",
        "    return d\n",
        "class TransformersSlowUD(object):\n",
        "  def __init__(self,bert):\n",
        "    import os\n",
        "    self.tokenizer=AutoTokenizer.from_pretrained(bert)\n",
        "    self.model=AutoModelForQuestionAnswering.from_pretrained(bert)\n",
        "    x=AutoModelForTokenClassification.from_pretrained\n",
        "    if os.path.isdir(bert):\n",
        "      d,t=x(os.path.join(bert,\"deprel\")),x(os.path.join(bert,\"tagger\"))\n",
        "    else:\n",
        "      from transformers.file_utils import hf_bucket_url\n",
        "      c=AutoConfig.from_pretrained(hf_bucket_url(bert,\"deprel/config.json\"))\n",
        "      d=x(hf_bucket_url(bert,\"deprel/pytorch_model.bin\"),config=c)\n",
        "      s=AutoConfig.from_pretrained(hf_bucket_url(bert,\"tagger/config.json\"))\n",
        "      t=x(hf_bucket_url(bert,\"tagger/pytorch_model.bin\"),config=s)\n",
        "    self.deprel=TaggerPipeline(model=d,tokenizer=self.tokenizer,\n",
        "      aggregation_strategy=\"simple\")\n",
        "    self.tagger=TaggerPipeline(model=t,tokenizer=self.tokenizer)\n",
        "  def __call__(self,text):\n",
        "    import numpy,torch,ufal.chu_liu_edmonds\n",
        "    w=[(t[\"start\"],t[\"end\"],t[\"entity_group\"]) for t in self.deprel(text)]\n",
        "    z,n={t[\"start\"]:t[\"entity\"].split(\"|\") for t in self.tagger(text)},len(w)\n",
        "    r,m=[text[s:e] for s,e,p in w],numpy.full((n+1,n+1),numpy.nan)\n",
        "    v,c=self.tokenizer(r,add_special_tokens=False)[\"input_ids\"],[]\n",
        "    for i,t in enumerate(v):\n",
        "      q=[self.tokenizer.cls_token_id]+t+[self.tokenizer.sep_token_id]\n",
        "      c.append([q]+v[0:i]+[[self.tokenizer.mask_token_id]]+v[i+1:]+[[q[-1]]])\n",
        "    b=[[len(sum(x[0:j+1],[])) for j in range(len(x))] for x in c]\n",
        "    d=self.model(input_ids=torch.tensor([sum(x,[]) for x in c]),\n",
        "      token_type_ids=torch.tensor([[0]*x[0]+[1]*(x[-1]-x[0]) for x in b]))\n",
        "    s,e=d.start_logits.tolist(),d.end_logits.tolist()\n",
        "    for i in range(n):\n",
        "      for j in range(n):\n",
        "        m[i+1,0 if i==j else j+1]=s[i][b[i][j]]+e[i][b[i][j+1]-1]\n",
        "    h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "    if [0 for i in h if i==0]!=[0]:\n",
        "      i=([p for s,e,p in w]+[\"root\"]).index(\"root\")\n",
        "      j=i+1 if i<n else numpy.nanargmax(m[:,0])\n",
        "      m[0:j,0]=m[j+1:,0]=numpy.nan\n",
        "      h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "    u=\"# text = \"+text.replace(\"\\n\",\" \")+\"\\n\"\n",
        "    for i,(s,e,p) in enumerate(w,1):\n",
        "      p=\"root\" if h[i]==0 else \"dep\" if p==\"root\" else p\n",
        "      u+=\"\\t\".join([str(i),r[i-1],\"_\",z[s][0][2:],\"_\",\"|\".join(z[s][1:]),\n",
        "        str(h[i]),p,\"_\",\"_\" if i<n and w[i][0]<e else \"SpaceAfter=No\"])+\"\\n\"\n",
        "    return u+\"\\n\"\n",
        "\n",
        "nlp=TransformersSlowUD(\"qa-model\")\n",
        "for f in [\"dev.conllu\",\"test.conllu\",\"question1-1.conllu\",\"question1-2.conllu\"]:\n",
        "  with open(f,\"r\",encoding=\"utf-8\") as r:\n",
        "    with open(\"result-\"+f,\"w\",encoding=\"utf-8\") as w:\n",
        "      for s in r.read().split(\"\\n\"):\n",
        "        if s.startswith(\"# text = \"):\n",
        "          w.write(nlp(s[9:]))\n",
        "  print(\"***\",f)\n",
        "  !python conll18_ud_eval.py -v $f result-$f"
      ]
    }
  ]
}
