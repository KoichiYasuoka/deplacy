{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"TransformersのQuestion Answeringを用いた係り受け解析器" },
    "kernelspec":{ "name":"python3" }
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# TransformersのQuestion Answeringを用いた係り受け解析器\n",
        "## 国語研長単位モデル[deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)\n",
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "ufal.chu-liu-edmondsで係り受け木を導出"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install transformers ufal.chu-liu-edmonds deplacy\n",
        "\n",
        "class TransformersUD(object):\n",
        "  def __init__(self,bert):\n",
        "    import os\n",
        "    from transformers import (AutoTokenizer,AutoModelForQuestionAnswering,\n",
        "      AutoModelForTokenClassification,AutoConfig,TokenClassificationPipeline)\n",
        "    self.tokenizer=AutoTokenizer.from_pretrained(bert)\n",
        "    self.model=AutoModelForQuestionAnswering.from_pretrained(bert)\n",
        "    x=AutoModelForTokenClassification.from_pretrained\n",
        "    if os.path.isdir(bert):\n",
        "      d,t=x(os.path.join(bert,\"deprel\")),x(os.path.join(bert,\"tagger\"))\n",
        "    else:\n",
        "      from transformers.file_utils import hf_bucket_url\n",
        "      c=AutoConfig.from_pretrained(hf_bucket_url(bert,\"deprel/config.json\"))\n",
        "      d=x(hf_bucket_url(bert,\"deprel/pytorch_model.bin\"),config=c)\n",
        "      s=AutoConfig.from_pretrained(hf_bucket_url(bert,\"tagger/config.json\"))\n",
        "      t=x(hf_bucket_url(bert,\"tagger/pytorch_model.bin\"),config=s)\n",
        "    self.deprel=TokenClassificationPipeline(model=d,tokenizer=self.tokenizer,\n",
        "      aggregation_strategy=\"simple\")\n",
        "    self.tagger=TokenClassificationPipeline(model=t,tokenizer=self.tokenizer)\n",
        "  def __call__(self,text):\n",
        "    import numpy,torch,ufal.chu_liu_edmonds\n",
        "    w=[(t[\"start\"],t[\"end\"],t[\"entity_group\"]) for t in self.deprel(text)]\n",
        "    z,n={t[\"start\"]:t[\"entity\"].split(\"|\") for t in self.tagger(text)},len(w)\n",
        "    r,m=[text[s:e] for s,e,p in w],numpy.full((n+1,n+1),numpy.nan)\n",
        "    v=self.tokenizer(r,add_special_tokens=False)[\"input_ids\"]\n",
        "    for i,t in enumerate(v):\n",
        "      q=[self.tokenizer.cls_token_id]+t+[self.tokenizer.sep_token_id]\n",
        "      c=[q]+v[0:i]+[[self.tokenizer.mask_token_id]]+v[i+1:]+[[q[-1]]]\n",
        "      b=[len(sum(c[0:j+1],[])) for j in range(len(c))]\n",
        "      d=self.model(input_ids=torch.tensor([sum(c,[])]),\n",
        "        token_type_ids=torch.tensor([[0]*b[0]+[1]*(b[-1]-b[0])]))\n",
        "      s,e=d.start_logits.tolist()[0],d.end_logits.tolist()[0]\n",
        "      for j in range(n):\n",
        "        m[i+1,0 if i==j else j+1]=s[b[j]]+e[b[j+1]-1]\n",
        "    h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "    u=\"# text = \"+text.replace(\"\\n\",\" \")+\"\\n\"\n",
        "    for i,(s,e,p) in enumerate(w,1):\n",
        "      u+=\"\\t\".join([str(i),r[i-1],\"_\",z[s][0][2:],\"_\",\"|\".join(z[s][1:]),\n",
        "        str(h[i]),p,\"_\",\"_\" if i<n and w[i][0]<e else \"SpaceAfter=No\"])+\"\\n\"\n",
        "    return u+\"\\n\"\n",
        "\n",
        "nlp=TransformersUD(\"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\")\n",
        "doc=nlp(\"全学年にわたって小学校の国語の教科書に挿し絵が用いられている\")\n",
        "import deplacy\n",
        "deplacy.render(doc,Japanese=True)\n",
        "deplacy.serve(doc,port=None)"
      ]
    }
  ]
}
