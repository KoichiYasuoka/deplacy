{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"roberta-classical-chinese-base-ud-goeswithによる句間リンク抽出(まだ開発中)" },
    "kernelspec":{ "name":"python3" }
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [roberta-classical-chinese-base-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-base-ud-goeswith)による句間リンク抽出(まだ開発中)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install transformers ufal.chu_liu_edmonds deplacy\n",
        "!pip install transformers ufal.chu_liu_edmonds deplacy\n",
        "text1,text2=\"不入虎穴\",\"不得虎子\"\n",
        "bert=\"KoichiYasuoka/roberta-classical-chinese-base-ud-goeswith\"\n",
        "def dsp(h,p,t,l):\n",
        "  import deplacy\n",
        "  u=\"# text = \"+\"\".join(t)+\"\\n\"\n",
        "  for i,j in enumerate(t,1):\n",
        "    q=l[p[i,h[i]]].split(\"|\")\n",
        "    u+=\"\\t\".join([str(i),j,\"_\",q[0],\"_\",\"|\".join(q[1:-1]),str(h[i]),q[-1],\"_\",\"SpaceAfter=No\"])+\"\\n\"\n",
        "  deplacy.serve(u+\"\\n\",port=None)\n",
        "from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
        "tokenizer=AutoTokenizer.from_pretrained(bert)\n",
        "model=AutoModelForTokenClassification.from_pretrained(bert)\n",
        "import numpy,torch,ufal.chu_liu_edmonds\n",
        "w=tokenizer([text1,text2],add_special_tokens=False,return_offsets_mapping=True)\n",
        "v=[tokenizer.cls_token_id]+sum(w[\"input_ids\"],[])+[tokenizer.sep_token_id]\n",
        "x=[v[0:i]+[tokenizer.mask_token_id]+v[i+1:]+[j] for i,j in enumerate(v[1:-1],1)]\n",
        "with torch.no_grad():\n",
        "  e=model(input_ids=torch.tensor(x)).logits.numpy()[:,1:-2,:]\n",
        "l=model.config.id2label\n",
        "r=[1 if i==0 else -1 if j.endswith(\"|root\") else 0 for i,j in sorted(l.items())]\n",
        "e+=numpy.where(numpy.add.outer(numpy.identity(e.shape[0]),r)==0,0,numpy.nan)\n",
        "m=numpy.full((e.shape[0]+1,e.shape[1]+1),numpy.nan)\n",
        "m[1:,1:]=numpy.nanmax(e,axis=2).transpose()\n",
        "p=numpy.zeros(m.shape)\n",
        "p[1:,1:]=numpy.nanargmax(e,axis=2).transpose()\n",
        "for i in range(1,m.shape[0]):\n",
        "  m[i,0],m[i,i],p[i,0]=m[i,i],numpy.nan,p[i,i]\n",
        "h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "t=sum([[y[s:e] for s,e in x if s<e] for x,y in zip(w[\"offset_mapping\"],[text1,text2])],[])\n",
        "dsp(h,p,t,l)\n",
        "mm=m.copy()\n",
        "n=len(w[0])+1\n",
        "mm[1:n,n:]=mm[n:,1:n]=numpy.nan\n",
        "h=ufal.chu_liu_edmonds.chu_liu_edmonds(mm)[0]\n",
        "dsp(h,p,t,l)\n",
        "x=[i for i,j in enumerate(h) if j==0]\n",
        "for i in x:\n",
        "  y=[j for j in x if j!=i]\n",
        "  mm[i,y]=m[i,y]\n",
        "h=ufal.chu_liu_edmonds.chu_liu_edmonds(mm)[0]\n",
        "dsp(h,p,t,l)"
      ]
    }
  ]
}
