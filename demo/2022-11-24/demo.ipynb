{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"한문교육연구소 제6회 초청강연회" },
    "kernelspec":{ "name":"python3" }
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [Universal Dependencies와 BERT/RoBERTa 모델을 통한 고전 중국어 정보처리](http://www.han-character.education/home/index.php/2022/11/18/20221118/)"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "## 古典中国語RoBERTaの穴埋めゲーム"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "fmp=pipeline(\"fill-mask\",\"KoichiYasuoka/roberta-classical-chinese-base-char\")\n",
        "prd=fmp(\"勞[MASK]者治人\")\n",
        "print(\"\\n\".join(\"{:8} {:.3f}\".format(t[\"token_str\"],t[\"score\"]) for t in prd))"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "## 系列ラベリングによる品詞付与・単語切り"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install deplacy transformers\n",
        "class SeqL(object):\n",
        "  def __init__(self,bert):\n",
        "    from transformers import pipeline\n",
        "    self.tagger=pipeline(task=\"ner\",model=bert)\n",
        "  def __call__(self,text):\n",
        "    w=[(t[\"start\"],t[\"end\"],t[\"entity\"]) for t in self.tagger(text)]\n",
        "    u=\"# text = \"+text.replace(\"\\n\",\" \")+\"\\n\"\n",
        "    for i,(s,e,p) in enumerate(w,1):\n",
        "      m,q=\"_\" if i<len(w) and e<w[i][0] else \"SpaceAfter=No\",p.split(\"|\")\n",
        "      f=\"_\" if p.find(\"=\")<0 else \"|\".join(t for t in q if t.find(\"=\")>0)\n",
        "      u+=\"\\t\".join([str(i),text[s:e],\"_\",q[0],\"_\",f,\"_\",\"_\",\"_\",m])+\"\\n\"\n",
        "    return u+\"\\n\"\n",
        "nlp=SeqL(\"KoichiYasuoka/roberta-classical-chinese-base-ud-goeswith\")\n",
        "doc=nlp(\"未聞好學者也\")\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "## 系列ラベリングによる隣接行列logits導出"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install transformers\n",
        "import torch,numpy\n",
        "from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
        "brt=\"KoichiYasuoka/roberta-classical-chinese-base-ud-goeswith\"\n",
        "txt=\"未聞好學者也\"\n",
        "tkz=AutoTokenizer.from_pretrained(brt)\n",
        "mdl=AutoModelForTokenClassification.from_pretrained(brt)\n",
        "v,l=tkz(txt,return_offsets_mapping=True),mdl.config.id2label\n",
        "w=[t for t,(s,e) in zip(v[\"input_ids\"],v[\"offset_mapping\"]) if s<e]\n",
        "u=[txt[s:e] for s,e in v[\"offset_mapping\"] if s<e]\n",
        "cls,msk,sep=tkz.cls_token_id,tkz.mask_token_id,tkz.sep_token_id\n",
        "x=[[cls]+w[:i]+[msk]+w[i+1:]+[sep,j] for i,j in enumerate(w)]\n",
        "with torch.no_grad():\n",
        "  m=mdl(input_ids=torch.tensor(x)).logits.numpy()[:,1:-2,:]\n",
        "r=[1 if i==0 else -1 if l[i].endswith(\"|root\") else 0 for i in range(len(l))]\n",
        "m+=numpy.where(numpy.add.outer(numpy.identity(m.shape[0]),r)==0,0,numpy.nan)\n",
        "g=mdl.config.label2id[\"X|_|goeswith\"]\n",
        "r=numpy.tri(m.shape[0])\n",
        "for i in range(r.shape[0]):\n",
        "  for j in range(i+2,r.shape[1]):\n",
        "    r[i,j]=r[i,j-1] if numpy.nanargmax(m[i,j-1])==g else 1\n",
        "m[:,:,g]+=numpy.where(r==0,0,numpy.nan)\n",
        "d,p=numpy.nanmax(m,axis=2),numpy.nanargmax(m,axis=2)\n",
        "print(\" \".join(x.rjust(12-len(x)) for x in u))\n",
        "for i,j in enumerate(u):\n",
        "  print(\"\\n\"+\" \".join(\"{:12.3f}\".format(x) for x in d[i]),\" \",j)\n",
        "  print(\" \".join(l[x].split(\"|\")[-1][:12].rjust(12) for x in p[i]))"
      ]
    }
  ]
}
