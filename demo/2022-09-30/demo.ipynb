{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"roberta-classical-chinese-base-ud-goeswithによる句間リンク(開発中)" },
    "kernelspec":{ "name":"python3" }
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [roberta-classical-chinese-base-ud-goeswith](https://huggingface.co/KoichiYasuoka/roberta-classical-chinese-base-ud-goeswith)による句間リンク(開発中)"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install transformers ufal.chu_liu_edmonds deplacy\n",
        "class UDintertexts(object):\n",
        "  def __init__(self,bert):\n",
        "    from transformers import AutoTokenizer,AutoModelForTokenClassification\n",
        "    self.tokenizer=AutoTokenizer.from_pretrained(bert)\n",
        "    self.model=AutoModelForTokenClassification.from_pretrained(bert)\n",
        "  def __call__(self,text1,text2):\n",
        "    import numpy,torch,ufal.chu_liu_edmonds\n",
        "    text=text1+text2\n",
        "    w=self.tokenizer(text,return_offsets_mapping=True)\n",
        "    v=w[\"input_ids\"]\n",
        "    n,n1,n2=len(v)-1,len(text1),len(text2)\n",
        "    with torch.no_grad():\n",
        "      d=self.model(input_ids=torch.tensor([v[0:i]+[self.tokenizer.mask_token_id]+v[i+1:]+[v[i]] for i in range(1,n)]))\n",
        "    e=d.logits.numpy()[:,1:n,:]\n",
        "    e[:,:,0]=numpy.nan\n",
        "    m=numpy.full((n,n),numpy.nan)\n",
        "    m[1:,1:]=numpy.nanmax(e,axis=2).transpose()\n",
        "    p=numpy.zeros((n,n))\n",
        "    p[1:,1:]=numpy.nanargmax(e,axis=2).transpose()\n",
        "    for i in range(1,n):\n",
        "      m[i,0],m[i,i],p[i,0]=m[i,i],numpy.nan,p[i,i]\n",
        "    mm=numpy.full((n,n),numpy.nan)\n",
        "    mm[1:n1+1,n1+1:]=m[1:n1+1,n1+1:]\n",
        "    mm[n1+1:,1:n1+1]=m[n1+1:,1:n1+1]\n",
        "    x,y=numpy.unravel_index(numpy.nanargmax(mm),mm.shape)\n",
        "    mm=numpy.full((n,n),numpy.nan)\n",
        "    mm[1:n1+1,1:n1+1]=m[1:n1+1,1:n1+1]\n",
        "    mm[n1+1:,n1+1:]=m[n1+1:,n1+1:]\n",
        "    mm[x,y]=m[x,y]\n",
        "    print(x,y,self.model.config.id2label[p[x,y]].split(\"|\")[-1])\n",
        "    mm[:,0]=m[:,0]\n",
        "    h=ufal.chu_liu_edmonds.chu_liu_edmonds(mm)[0]\n",
        "    u=\"# text = \"+text+\"\\n\"\n",
        "    v=[(s,e) for s,e in w[\"offset_mapping\"] if s<e]\n",
        "    for i,(s,e) in enumerate(v,1):\n",
        "      q=self.model.config.id2label[p[i,h[i]]].split(\"|\")\n",
        "      u+=\"\\t\".join([str(i),text[s:e],\"_\",q[0],\"_\",\"|\".join(q[1:-1]),str(h[i]),q[-1],\"_\",\"_\" if i<len(v) and e<v[i][0] else \"SpaceAfter=No\"])+\"\\n\"\n",
        "    return u+\"\\n\"\n",
        "\n",
        "nlp=UDintertexts(\"KoichiYasuoka/roberta-classical-chinese-base-ud-goeswith\")\n",
        "doc=nlp(\"不入虎穴\",\"不得虎子\")\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    }
  ]
}
