{
  "nbformat":4,
  "nbformat_minor":0,
  "metadata":{
    "colab":{ "name":"青空文庫DeBERTaモデルによる国語研長単位係り受け解析" },
    "kernelspec":{ "name":"python3" }
  },
  "cells":[
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "# [青空文庫DeBERTaモデルによる国語研長単位係り受け解析](http://hdl.handle.net/2433/275409)\n"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "## esuparモデル[deberta-base-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-luw-upos)\n"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install esupar deplacy\n",
        "import esupar\n",
        "nlp=esupar.load(\"KoichiYasuoka/deberta-base-japanese-luw-upos\")\n",
        "doc=nlp(\"世界中が刮目している\")\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    },
    {
      "cell_type":"markdown",
      "metadata":{ "colab_type":"text" },
      "source":[
        "## TransformersUDモデル[deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head)\n"
      ]
    },
    {
      "cell_type":"code",
      "metadata":{ "colab_type":"code" },
      "source":[
        "!pip install transformers ufal.chu-liu-edmonds deplacy\n",
        "\n",
        "class TransformersUD(object):\n",
        "  def __init__(self,bert):\n",
        "    import os\n",
        "    from transformers import (AutoTokenizer,AutoModelForQuestionAnswering,\n",
        "      AutoModelForTokenClassification,AutoConfig,TokenClassificationPipeline)\n",
        "    self.tokenizer=AutoTokenizer.from_pretrained(bert)\n",
        "    self.model=AutoModelForQuestionAnswering.from_pretrained(bert)\n",
        "    x=AutoModelForTokenClassification.from_pretrained\n",
        "    if os.path.isdir(bert):\n",
        "      d,t=x(os.path.join(bert,\"deprel\")),x(os.path.join(bert,\"tagger\"))\n",
        "    else:\n",
        "      from transformers.file_utils import hf_bucket_url\n",
        "      c=AutoConfig.from_pretrained(hf_bucket_url(bert,\"deprel/config.json\"))\n",
        "      d=x(hf_bucket_url(bert,\"deprel/pytorch_model.bin\"),config=c)\n",
        "      s=AutoConfig.from_pretrained(hf_bucket_url(bert,\"tagger/config.json\"))\n",
        "      t=x(hf_bucket_url(bert,\"tagger/pytorch_model.bin\"),config=s)\n",
        "    self.deprel=TokenClassificationPipeline(model=d,tokenizer=self.tokenizer,\n",
        "      aggregation_strategy=\"simple\")\n",
        "    self.tagger=TokenClassificationPipeline(model=t,tokenizer=self.tokenizer)\n",
        "  def __call__(self,text):\n",
        "    import numpy,torch,ufal.chu_liu_edmonds\n",
        "    w=[(t[\"start\"],t[\"end\"],t[\"entity_group\"]) for t in self.deprel(text)]\n",
        "    z,n={t[\"start\"]:t[\"entity\"].split(\"|\") for t in self.tagger(text)},len(w)\n",
        "    r,m=[text[s:e] for s,e,p in w],numpy.full((n+1,n+1),numpy.nan)\n",
        "    v,c=self.tokenizer(r,add_special_tokens=False)[\"input_ids\"],[]\n",
        "    for i,t in enumerate(v):\n",
        "      q=[self.tokenizer.cls_token_id]+t+[self.tokenizer.sep_token_id]\n",
        "      c.append([q]+v[0:i]+[[self.tokenizer.mask_token_id]]+v[i+1:]+[[q[-1]]])\n",
        "    b=[[len(sum(x[0:j+1],[])) for j in range(len(x))] for x in c]\n",
        "    d=self.model(input_ids=torch.tensor([sum(x,[]) for x in c]),\n",
        "      token_type_ids=torch.tensor([[0]*x[0]+[1]*(x[-1]-x[0]) for x in b]))\n",
        "    s,e=d.start_logits.tolist(),d.end_logits.tolist()\n",
        "    for i in range(n):\n",
        "      for j in range(n):\n",
        "        m[i+1,0 if i==j else j+1]=s[i][b[i][j]]+e[i][b[i][j+1]-1]\n",
        "    h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "    if [0 for i in h if i==0]!=[0]:\n",
        "      i=([p for s,e,p in w]+[\"root\"]).index(\"root\")\n",
        "      j=i+1 if i<n else numpy.nanargmax(m[:,0])\n",
        "      m[0:j,0]=m[j+1:,0]=numpy.nan\n",
        "      h=ufal.chu_liu_edmonds.chu_liu_edmonds(m)[0]\n",
        "    u=\"# text = \"+text.replace(\"\\n\",\" \")+\"\\n\"\n",
        "    for i,(s,e,p) in enumerate(w,1):\n",
        "      p=\"root\" if h[i]==0 else \"dep\" if p==\"root\" else p\n",
        "      u+=\"\\t\".join([str(i),r[i-1],\"_\",z[s][0][2:],\"_\",\"|\".join(z[s][1:]),\n",
        "        str(h[i]),p,\"_\",\"_\" if i<n and e<w[i][0] else \"SpaceAfter=No\"])+\"\\n\"\n",
        "    return u+\"\\n\"\n",
        "\n",
        "nlp=TransformersUD(\"KoichiYasuoka/deberta-base-japanese-aozora-ud-head\")\n",
        "doc=nlp(\"世界中が刮目している\")\n",
        "import deplacy\n",
        "deplacy.serve(doc,port=None)"
      ]
    }
  ]
}
